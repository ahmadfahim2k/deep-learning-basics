{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpRkAlxQwQjy"
      },
      "source": [
        "# Problem Statement: **BONUS EXERCISE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvpy57daLojm"
      },
      "source": [
        "Imports and CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hOiJAjpnklhJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYMNU-bmZXl"
      },
      "source": [
        "Problem 1: **Gradient Descent for Demand Forecasting at AtliQ**\n",
        "\n",
        "AtliQ wants to optimize the prediction of regional product demands using gradient descent.\n",
        "\n",
        "Assume the loss function is\n",
        "\n",
        "$$L(w)=(w‚àí4)^2$$\n",
        "\n",
        "where **w** is a weight parameter initialized at 0.\n",
        "\n",
        "**Write code to:**\n",
        "\n",
        "* Perform 10 iterations of gradient descent using a learning rate of 0.1.\n",
        "\n",
        "* Print the weight **w** at each step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bhYyNy9mndvd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: w = 0.8000\n",
            "Step 2: w = 1.4400\n",
            "Step 3: w = 1.9520\n",
            "Step 4: w = 2.3616\n",
            "Step 5: w = 2.6893\n",
            "Step 6: w = 2.9514\n",
            "Step 7: w = 3.1611\n",
            "Step 8: w = 3.3289\n",
            "Step 9: w = 3.4631\n",
            "Step 10: w = 3.5705\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.1\n",
        "w = 0.0\n",
        "\n",
        "for i in range (10):\n",
        "  gradient = 2*(w-4)\n",
        "  w = w - learning_rate * gradient\n",
        "  print(f\"Step {i+1}: w = {w:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xADdX0ULpDhP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40THnZYtoOkV"
      },
      "source": [
        "Problem 2: **Momentum for Contour Navigation in AtliQ's Supply Chain**\n",
        "\n",
        "AtliQ's supply chain optimization problem is represented by a contour map of a quadratic function:\n",
        "\n",
        "$$f(x,y)=x^2 +3y^2$$\n",
        "\n",
        "Write a code to implement gradient descent (5 iterations) with momentum to minimize this function.\n",
        "\n",
        "Use:\n",
        "* Initial point (x, y) = (2, 2)\n",
        "* Learning rate (Œ∑) = 0.1\n",
        "* Momentum Coefficient (Œ≤)) = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z2wHwhiHpCwf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: x = 1.9600, y = 1.8800\n",
            "Step 2: x = 1.8848, y = 1.6592\n",
            "Step 3: x = 1.7794, y = 1.3609\n",
            "Step 4: x = 1.6490, y = 1.0108\n",
            "Step 5: x = 1.4986, y = 0.6351\n"
          ]
        }
      ],
      "source": [
        "def gradient(x, y):\n",
        "  return (2*x, 6*y)\n",
        "\n",
        "x, y = 2,2\n",
        "learning_rate = 0.1\n",
        "momentum = 0.9\n",
        "vx, vy = 0.0, 0.0 # initialized velocity\n",
        "\n",
        "for i in range(5):\n",
        "  dx, dy = gradient(x, y)\n",
        "  vx = momentum*vx + (1-momentum)*dx\n",
        "  vy = momentum*vy + (1-momentum)*dy\n",
        "  x = x - learning_rate*vx\n",
        "  y = y - learning_rate*vy\n",
        "  print(f\"Step {i+1}: x = {x:.4f}, y = {y:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2tQRlsUqLvx"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJjaxm6kqNKE"
      },
      "source": [
        "Problem 3: **RMS Prop for AtliQ's Dynamic Pricing Optimization**\n",
        "\n",
        "AtliQ's AI model adjusts product prices dynamically. Implement the RMSProp optimizer for minimizing the function:\n",
        "\n",
        "$$f(w) = w^2 + 5$$\n",
        "\n",
        "Use:\n",
        "\n",
        "* Initial weight (ùë§) = 5.0\n",
        "* Learning rate (Œ∑) = 0.01\n",
        "* Momentum Coefficient(Œ≤)=0.9\n",
        "\n",
        "\n",
        "Run the optimization for 15 iterations and print the weight updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4GjcPpNKrwzm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: w = 4.9684\n",
            "Step 2: w = 4.9455\n",
            "Step 3: w = 4.9264\n",
            "Step 4: w = 4.9094\n",
            "Step 5: w = 4.8939\n",
            "Step 6: w = 4.8794\n",
            "Step 7: w = 4.8657\n",
            "Step 8: w = 4.8526\n",
            "Step 9: w = 4.8400\n",
            "Step 10: w = 4.8277\n",
            "Step 11: w = 4.8158\n",
            "Step 12: w = 4.8041\n",
            "Step 13: w = 4.7927\n",
            "Step 14: w = 4.7814\n",
            "Step 15: w = 4.7704\n"
          ]
        }
      ],
      "source": [
        "def gradient(w):\n",
        "  return 2*w\n",
        "\n",
        "w = 5.0\n",
        "learning_rate = 0.01\n",
        "beta = 0.9\n",
        "epsilon = 1e-8\n",
        "squared_gradient_average = 0.0 # initialized squared gradient average\n",
        "\n",
        "for i in range(15):\n",
        "  grad = gradient(w)\n",
        "  squared_gradient_average = squared_gradient_average * beta + (1-beta)*grad**2\n",
        "  w = w - (learning_rate * grad) / ((squared_gradient_average + epsilon)**0.5)\n",
        "  print(f\"Step {i+1}: w = {w:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0tb736Ysolc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87npAc9ysqOU"
      },
      "source": [
        "Problem 4: **Adam Optimizer for AtliQ AI Models**\n",
        "\n",
        "AtliQ is training an AI model to recommend warehouse restocking schedules. Use the Adam optimizer to minimize the function:\n",
        "\n",
        "$$f(x) = x^4 - 3x^3 + 2$$\n",
        "\n",
        "Write code to:\n",
        "\n",
        "* Initialize x = 3.0\n",
        "\n",
        "Run the optimizations for 19 iterations (starting from 1) with:\n",
        "* Learning rate (Œ∑) = 0.01\n",
        "* Momentum Coefficients: Œ≤1 = 0.9, Œ≤2 = 0.09\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dQjK7sR4twzm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: x = 2.9900\n",
            "Step 2: x = 2.9799\n",
            "Step 3: x = 2.9697\n",
            "Step 4: x = 2.9595\n",
            "Step 5: x = 2.9491\n",
            "Step 6: x = 2.9387\n",
            "Step 7: x = 2.9281\n",
            "Step 8: x = 2.9174\n",
            "Step 9: x = 2.9067\n",
            "Step 10: x = 2.8958\n",
            "Step 11: x = 2.8849\n",
            "Step 12: x = 2.8739\n",
            "Step 13: x = 2.8627\n",
            "Step 14: x = 2.8515\n",
            "Step 15: x = 2.8401\n",
            "Step 16: x = 2.8287\n",
            "Step 17: x = 2.8171\n",
            "Step 18: x = 2.8054\n",
            "Step 19: x = 2.7937\n"
          ]
        }
      ],
      "source": [
        "def gradient(x):\n",
        "  return 4*x**3 - 9*x**2\n",
        "\n",
        "x = 3.0\n",
        "learning_rate = 0.01\n",
        "beta1, beta2 = 0.9, 0.09\n",
        "epsilon = 1e-8\n",
        "first_moment, second_moment = 0.0, 0.0 # initialized first and second moment\n",
        "\n",
        "for t in range(1, 20):\n",
        "  grad = gradient(x)\n",
        "  m = beta1*first_moment + (1-beta1)*grad\n",
        "  v = beta2*second_moment + (1-beta2)*grad**2\n",
        "  m_hat = m / (1 - beta1**t)\n",
        "  v_hat = v / (1 - beta2**t)\n",
        "  x = x - (learning_rate * m_hat) / ((v_hat+epsilon)**0.5)\n",
        "  first_moment, second_moment = m, v\n",
        "  print(f\"Step {t}: x = {x:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "953S3HD64qNm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
